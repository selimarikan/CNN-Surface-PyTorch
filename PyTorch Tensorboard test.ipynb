{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.misc \n",
    "from io import BytesIO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    \n",
    "    def __init__(self, log_dir):\n",
    "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
    "        self.writer = tf.summary.FileWriter(log_dir)\n",
    "\n",
    "    def scalar_summary(self, tag, value, step):\n",
    "        \"\"\"Log a scalar variable.\"\"\"\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "\n",
    "    def image_summary(self, tag, images, step):\n",
    "        \"\"\"Log a list of images.\"\"\"\n",
    "\n",
    "        img_summaries = []\n",
    "        for i, img in enumerate(images):\n",
    "            # Write the image to a string\n",
    "            try:\n",
    "                s = StringIO()\n",
    "            except:\n",
    "                s = BytesIO()\n",
    "            scipy.misc.toimage(img).save(s, format=\"png\")\n",
    "\n",
    "            # Create an Image object\n",
    "            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n",
    "                                       height=img.shape[0],\n",
    "                                       width=img.shape[1])\n",
    "            # Create a Summary value\n",
    "            img_summaries.append(tf.Summary.Value(tag='%s/%d' % (tag, i), image=img_sum))\n",
    "\n",
    "        # Create and write Summary\n",
    "        summary = tf.Summary(value=img_summaries)\n",
    "        self.writer.add_summary(summary, step)\n",
    "        \n",
    "    def histo_summary(self, tag, values, step, bins=1000):\n",
    "        \"\"\"Log a histogram of the tensor of values.\"\"\"\n",
    "\n",
    "        # Create a histogram using numpy\n",
    "        counts, bin_edges = np.histogram(values, bins=bins)\n",
    "\n",
    "        # Fill the fields of the histogram proto\n",
    "        hist = tf.HistogramProto()\n",
    "        hist.min = float(np.min(values))\n",
    "        hist.max = float(np.max(values))\n",
    "        hist.num = int(np.prod(values.shape))\n",
    "        hist.sum = float(np.sum(values))\n",
    "        hist.sum_squares = float(np.sum(values**2))\n",
    "\n",
    "        # Drop the start of the first bin\n",
    "        bin_edges = bin_edges[1:]\n",
    "\n",
    "        # Add bin edges and counts\n",
    "        for edge in bin_edges:\n",
    "            hist.bucket_limit.append(edge)\n",
    "        for c in counts:\n",
    "            hist.bucket.append(c)\n",
    "\n",
    "        # Create and write Summary\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "        self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MNIST Dataset \n",
    "dataset = dsets.MNIST(root='./data', \n",
    "                      train=True, \n",
    "                      transform=transforms.ToTensor(),  \n",
    "                      download=True)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "data_loader = torch.utils.data.DataLoader(dataset=dataset, \n",
    "                                          batch_size=100, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_np(x):\n",
    "    return x.data.cpu().numpy()\n",
    "\n",
    "def to_var(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Neural Network Model (1 hidden layer)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=500, num_classes=10):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Net()\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the logger\n",
    "logger = Logger('./logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)  \n",
    "\n",
    "data_iter = iter(data_loader)\n",
    "iter_per_epoch = len(data_loader)\n",
    "total_step = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [100/50000], Loss: 2.2013, Acc: 0.52\n",
      "Step [200/50000], Loss: 2.0939, Acc: 0.66\n",
      "Step [300/50000], Loss: 1.9519, Acc: 0.80\n",
      "Step [400/50000], Loss: 1.8311, Acc: 0.79\n",
      "Step [500/50000], Loss: 1.6748, Acc: 0.85\n",
      "Step [600/50000], Loss: 1.5053, Acc: 0.87\n",
      "Step [700/50000], Loss: 1.4880, Acc: 0.78\n",
      "Step [800/50000], Loss: 1.3246, Acc: 0.85\n",
      "Step [900/50000], Loss: 1.3188, Acc: 0.77\n",
      "Step [1000/50000], Loss: 1.2063, Acc: 0.80\n",
      "Step [1100/50000], Loss: 1.1358, Acc: 0.82\n",
      "Step [1200/50000], Loss: 0.9550, Acc: 0.87\n",
      "Step [1300/50000], Loss: 0.8603, Acc: 0.92\n",
      "Step [1400/50000], Loss: 0.9213, Acc: 0.83\n",
      "Step [1500/50000], Loss: 0.9349, Acc: 0.82\n",
      "Step [1600/50000], Loss: 0.7830, Acc: 0.87\n",
      "Step [1700/50000], Loss: 0.7285, Acc: 0.87\n",
      "Step [1800/50000], Loss: 0.7723, Acc: 0.83\n",
      "Step [1900/50000], Loss: 0.7345, Acc: 0.87\n",
      "Step [2000/50000], Loss: 0.7791, Acc: 0.80\n",
      "Step [2100/50000], Loss: 0.5747, Acc: 0.91\n",
      "Step [2200/50000], Loss: 0.6322, Acc: 0.92\n",
      "Step [2300/50000], Loss: 0.5405, Acc: 0.90\n",
      "Step [2400/50000], Loss: 0.6486, Acc: 0.87\n",
      "Step [2500/50000], Loss: 0.5494, Acc: 0.84\n",
      "Step [2600/50000], Loss: 0.5303, Acc: 0.89\n",
      "Step [2700/50000], Loss: 0.6002, Acc: 0.82\n",
      "Step [2800/50000], Loss: 0.6610, Acc: 0.79\n",
      "Step [2900/50000], Loss: 0.5862, Acc: 0.85\n",
      "Step [3000/50000], Loss: 0.5741, Acc: 0.88\n",
      "Step [3100/50000], Loss: 0.4741, Acc: 0.91\n",
      "Step [3200/50000], Loss: 0.5045, Acc: 0.89\n",
      "Step [3300/50000], Loss: 0.3905, Acc: 0.94\n",
      "Step [3400/50000], Loss: 0.5283, Acc: 0.85\n",
      "Step [3500/50000], Loss: 0.4811, Acc: 0.86\n",
      "Step [3600/50000], Loss: 0.3987, Acc: 0.90\n",
      "Step [3700/50000], Loss: 0.4054, Acc: 0.90\n",
      "Step [3800/50000], Loss: 0.4526, Acc: 0.88\n",
      "Step [3900/50000], Loss: 0.4189, Acc: 0.90\n",
      "Step [4000/50000], Loss: 0.5741, Acc: 0.83\n",
      "Step [4100/50000], Loss: 0.4895, Acc: 0.84\n",
      "Step [4200/50000], Loss: 0.3589, Acc: 0.95\n",
      "Step [4300/50000], Loss: 0.3534, Acc: 0.90\n",
      "Step [4400/50000], Loss: 0.3186, Acc: 0.93\n",
      "Step [4500/50000], Loss: 0.5751, Acc: 0.86\n",
      "Step [4600/50000], Loss: 0.4141, Acc: 0.90\n",
      "Step [4700/50000], Loss: 0.4310, Acc: 0.89\n",
      "Step [4800/50000], Loss: 0.4147, Acc: 0.86\n",
      "Step [4900/50000], Loss: 0.3869, Acc: 0.87\n",
      "Step [5000/50000], Loss: 0.3053, Acc: 0.94\n",
      "Step [5100/50000], Loss: 0.4089, Acc: 0.87\n",
      "Step [5200/50000], Loss: 0.2951, Acc: 0.94\n",
      "Step [5300/50000], Loss: 0.3815, Acc: 0.87\n",
      "Step [5400/50000], Loss: 0.3878, Acc: 0.89\n",
      "Step [5500/50000], Loss: 0.2977, Acc: 0.93\n",
      "Step [5600/50000], Loss: 0.3045, Acc: 0.91\n",
      "Step [5700/50000], Loss: 0.3474, Acc: 0.92\n",
      "Step [5800/50000], Loss: 0.5903, Acc: 0.83\n",
      "Step [5900/50000], Loss: 0.2471, Acc: 0.95\n",
      "Step [6000/50000], Loss: 0.3864, Acc: 0.88\n",
      "Step [6100/50000], Loss: 0.4705, Acc: 0.85\n",
      "Step [6200/50000], Loss: 0.2462, Acc: 0.96\n",
      "Step [6300/50000], Loss: 0.4220, Acc: 0.91\n",
      "Step [6400/50000], Loss: 0.3632, Acc: 0.91\n",
      "Step [6500/50000], Loss: 0.4492, Acc: 0.89\n",
      "Step [6600/50000], Loss: 0.5135, Acc: 0.87\n",
      "Step [6700/50000], Loss: 0.2950, Acc: 0.93\n",
      "Step [6800/50000], Loss: 0.3840, Acc: 0.89\n",
      "Step [6900/50000], Loss: 0.2564, Acc: 0.93\n",
      "Step [7000/50000], Loss: 0.4489, Acc: 0.87\n",
      "Step [7100/50000], Loss: 0.3174, Acc: 0.93\n",
      "Step [7200/50000], Loss: 0.3312, Acc: 0.91\n",
      "Step [7300/50000], Loss: 0.3119, Acc: 0.94\n",
      "Step [7400/50000], Loss: 0.4604, Acc: 0.89\n",
      "Step [7500/50000], Loss: 0.3796, Acc: 0.88\n",
      "Step [7600/50000], Loss: 0.3120, Acc: 0.93\n",
      "Step [7700/50000], Loss: 0.2126, Acc: 0.94\n",
      "Step [7800/50000], Loss: 0.3420, Acc: 0.91\n",
      "Step [7900/50000], Loss: 0.2827, Acc: 0.92\n",
      "Step [8000/50000], Loss: 0.3530, Acc: 0.92\n",
      "Step [8100/50000], Loss: 0.3171, Acc: 0.92\n",
      "Step [8200/50000], Loss: 0.3091, Acc: 0.93\n",
      "Step [8300/50000], Loss: 0.2680, Acc: 0.94\n",
      "Step [8400/50000], Loss: 0.3948, Acc: 0.91\n",
      "Step [8500/50000], Loss: 0.2627, Acc: 0.92\n",
      "Step [8600/50000], Loss: 0.5097, Acc: 0.87\n",
      "Step [8700/50000], Loss: 0.2520, Acc: 0.94\n",
      "Step [8800/50000], Loss: 0.2847, Acc: 0.90\n",
      "Step [8900/50000], Loss: 0.3535, Acc: 0.88\n",
      "Step [9000/50000], Loss: 0.3824, Acc: 0.88\n",
      "Step [9100/50000], Loss: 0.2340, Acc: 0.95\n",
      "Step [9200/50000], Loss: 0.3180, Acc: 0.94\n",
      "Step [9300/50000], Loss: 0.3778, Acc: 0.91\n",
      "Step [9400/50000], Loss: 0.3011, Acc: 0.93\n",
      "Step [9500/50000], Loss: 0.3009, Acc: 0.93\n",
      "Step [9600/50000], Loss: 0.4412, Acc: 0.85\n",
      "Step [9700/50000], Loss: 0.3120, Acc: 0.92\n",
      "Step [9800/50000], Loss: 0.3203, Acc: 0.92\n",
      "Step [9900/50000], Loss: 0.2627, Acc: 0.94\n",
      "Step [10000/50000], Loss: 0.1951, Acc: 0.95\n",
      "Step [10100/50000], Loss: 0.2909, Acc: 0.92\n",
      "Step [10200/50000], Loss: 0.1597, Acc: 0.96\n",
      "Step [10300/50000], Loss: 0.2840, Acc: 0.92\n",
      "Step [10400/50000], Loss: 0.2374, Acc: 0.93\n",
      "Step [10500/50000], Loss: 0.2271, Acc: 0.96\n",
      "Step [10600/50000], Loss: 0.3144, Acc: 0.91\n",
      "Step [10700/50000], Loss: 0.2778, Acc: 0.91\n",
      "Step [10800/50000], Loss: 0.2375, Acc: 0.92\n",
      "Step [10900/50000], Loss: 0.2401, Acc: 0.93\n",
      "Step [11000/50000], Loss: 0.2483, Acc: 0.93\n",
      "Step [11100/50000], Loss: 0.3181, Acc: 0.91\n",
      "Step [11200/50000], Loss: 0.2826, Acc: 0.93\n",
      "Step [11300/50000], Loss: 0.2718, Acc: 0.90\n",
      "Step [11400/50000], Loss: 0.3235, Acc: 0.93\n",
      "Step [11500/50000], Loss: 0.2982, Acc: 0.94\n",
      "Step [11600/50000], Loss: 0.2400, Acc: 0.95\n",
      "Step [11700/50000], Loss: 0.2580, Acc: 0.92\n",
      "Step [11800/50000], Loss: 0.3063, Acc: 0.90\n",
      "Step [11900/50000], Loss: 0.3167, Acc: 0.89\n",
      "Step [12000/50000], Loss: 0.2972, Acc: 0.91\n",
      "Step [12100/50000], Loss: 0.3110, Acc: 0.91\n",
      "Step [12200/50000], Loss: 0.4360, Acc: 0.90\n",
      "Step [12300/50000], Loss: 0.3332, Acc: 0.87\n",
      "Step [12400/50000], Loss: 0.2090, Acc: 0.94\n",
      "Step [12500/50000], Loss: 0.1493, Acc: 0.95\n",
      "Step [12600/50000], Loss: 0.2670, Acc: 0.92\n",
      "Step [12700/50000], Loss: 0.3341, Acc: 0.93\n",
      "Step [12800/50000], Loss: 0.2734, Acc: 0.92\n",
      "Step [12900/50000], Loss: 0.1781, Acc: 0.95\n",
      "Step [13000/50000], Loss: 0.2469, Acc: 0.93\n",
      "Step [13100/50000], Loss: 0.2502, Acc: 0.92\n",
      "Step [13200/50000], Loss: 0.4138, Acc: 0.90\n",
      "Step [13300/50000], Loss: 0.4744, Acc: 0.89\n",
      "Step [13400/50000], Loss: 0.2960, Acc: 0.94\n",
      "Step [13500/50000], Loss: 0.2207, Acc: 0.93\n",
      "Step [13600/50000], Loss: 0.2602, Acc: 0.93\n",
      "Step [13700/50000], Loss: 0.3625, Acc: 0.86\n",
      "Step [13800/50000], Loss: 0.2779, Acc: 0.95\n",
      "Step [13900/50000], Loss: 0.3204, Acc: 0.90\n",
      "Step [14000/50000], Loss: 0.3146, Acc: 0.94\n",
      "Step [14100/50000], Loss: 0.2002, Acc: 0.94\n",
      "Step [14200/50000], Loss: 0.2774, Acc: 0.93\n",
      "Step [14300/50000], Loss: 0.1999, Acc: 0.96\n",
      "Step [14400/50000], Loss: 0.2094, Acc: 0.92\n",
      "Step [14500/50000], Loss: 0.2511, Acc: 0.92\n",
      "Step [14600/50000], Loss: 0.2109, Acc: 0.92\n",
      "Step [14700/50000], Loss: 0.3202, Acc: 0.93\n",
      "Step [14800/50000], Loss: 0.2197, Acc: 0.95\n",
      "Step [14900/50000], Loss: 0.3006, Acc: 0.89\n",
      "Step [15000/50000], Loss: 0.3337, Acc: 0.84\n",
      "Step [15100/50000], Loss: 0.1698, Acc: 0.97\n",
      "Step [15200/50000], Loss: 0.2952, Acc: 0.92\n",
      "Step [15300/50000], Loss: 0.2443, Acc: 0.93\n",
      "Step [15400/50000], Loss: 0.2544, Acc: 0.94\n",
      "Step [15500/50000], Loss: 0.2489, Acc: 0.93\n",
      "Step [15600/50000], Loss: 0.1927, Acc: 0.94\n",
      "Step [15700/50000], Loss: 0.2325, Acc: 0.94\n",
      "Step [15800/50000], Loss: 0.1779, Acc: 0.99\n",
      "Step [15900/50000], Loss: 0.1701, Acc: 0.94\n",
      "Step [16000/50000], Loss: 0.1838, Acc: 0.96\n",
      "Step [16100/50000], Loss: 0.3686, Acc: 0.93\n",
      "Step [16200/50000], Loss: 0.2168, Acc: 0.93\n",
      "Step [16300/50000], Loss: 0.3010, Acc: 0.91\n",
      "Step [16400/50000], Loss: 0.2782, Acc: 0.88\n",
      "Step [16500/50000], Loss: 0.2284, Acc: 0.95\n",
      "Step [16600/50000], Loss: 0.3150, Acc: 0.91\n",
      "Step [16700/50000], Loss: 0.2543, Acc: 0.93\n",
      "Step [16800/50000], Loss: 0.2806, Acc: 0.91\n",
      "Step [16900/50000], Loss: 0.1819, Acc: 0.96\n",
      "Step [17000/50000], Loss: 0.2342, Acc: 0.91\n",
      "Step [17100/50000], Loss: 0.2217, Acc: 0.95\n",
      "Step [17200/50000], Loss: 0.2990, Acc: 0.94\n",
      "Step [17300/50000], Loss: 0.3102, Acc: 0.94\n",
      "Step [17400/50000], Loss: 0.1715, Acc: 0.96\n",
      "Step [17500/50000], Loss: 0.1406, Acc: 0.97\n",
      "Step [17600/50000], Loss: 0.2763, Acc: 0.94\n",
      "Step [17700/50000], Loss: 0.2713, Acc: 0.90\n",
      "Step [17800/50000], Loss: 0.2202, Acc: 0.95\n",
      "Step [17900/50000], Loss: 0.3802, Acc: 0.88\n",
      "Step [18000/50000], Loss: 0.2297, Acc: 0.93\n",
      "Step [18100/50000], Loss: 0.2489, Acc: 0.92\n",
      "Step [18200/50000], Loss: 0.2344, Acc: 0.92\n",
      "Step [18300/50000], Loss: 0.1729, Acc: 0.96\n",
      "Step [18400/50000], Loss: 0.1867, Acc: 0.96\n",
      "Step [18500/50000], Loss: 0.1878, Acc: 0.94\n",
      "Step [18600/50000], Loss: 0.1608, Acc: 0.95\n",
      "Step [18700/50000], Loss: 0.2773, Acc: 0.92\n",
      "Step [18800/50000], Loss: 0.3341, Acc: 0.89\n",
      "Step [18900/50000], Loss: 0.2764, Acc: 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [19000/50000], Loss: 0.1774, Acc: 0.94\n",
      "Step [19100/50000], Loss: 0.1820, Acc: 0.95\n",
      "Step [19200/50000], Loss: 0.2531, Acc: 0.89\n",
      "Step [19300/50000], Loss: 0.2196, Acc: 0.91\n",
      "Step [19400/50000], Loss: 0.1828, Acc: 0.96\n",
      "Step [19500/50000], Loss: 0.1662, Acc: 0.95\n",
      "Step [19600/50000], Loss: 0.1662, Acc: 0.95\n",
      "Step [19700/50000], Loss: 0.3422, Acc: 0.91\n",
      "Step [19800/50000], Loss: 0.1922, Acc: 0.94\n",
      "Step [19900/50000], Loss: 0.2694, Acc: 0.93\n",
      "Step [20000/50000], Loss: 0.1457, Acc: 0.97\n",
      "Step [20100/50000], Loss: 0.1884, Acc: 0.96\n",
      "Step [20200/50000], Loss: 0.2385, Acc: 0.92\n",
      "Step [20300/50000], Loss: 0.1596, Acc: 0.96\n",
      "Step [20400/50000], Loss: 0.2116, Acc: 0.94\n",
      "Step [20500/50000], Loss: 0.2128, Acc: 0.95\n",
      "Step [20600/50000], Loss: 0.2303, Acc: 0.93\n",
      "Step [20700/50000], Loss: 0.1924, Acc: 0.95\n",
      "Step [20800/50000], Loss: 0.2406, Acc: 0.93\n",
      "Step [20900/50000], Loss: 0.2893, Acc: 0.89\n",
      "Step [21000/50000], Loss: 0.2244, Acc: 0.94\n",
      "Step [21100/50000], Loss: 0.1806, Acc: 0.98\n",
      "Step [21200/50000], Loss: 0.1253, Acc: 0.98\n",
      "Step [21300/50000], Loss: 0.2205, Acc: 0.95\n",
      "Step [21400/50000], Loss: 0.1055, Acc: 0.98\n",
      "Step [21500/50000], Loss: 0.1343, Acc: 0.95\n",
      "Step [21600/50000], Loss: 0.1012, Acc: 0.97\n",
      "Step [21700/50000], Loss: 0.2256, Acc: 0.93\n",
      "Step [21800/50000], Loss: 0.2102, Acc: 0.93\n",
      "Step [21900/50000], Loss: 0.2269, Acc: 0.93\n",
      "Step [22000/50000], Loss: 0.2161, Acc: 0.97\n",
      "Step [22100/50000], Loss: 0.1691, Acc: 0.95\n",
      "Step [22200/50000], Loss: 0.0926, Acc: 0.99\n",
      "Step [22300/50000], Loss: 0.2117, Acc: 0.93\n",
      "Step [22400/50000], Loss: 0.2883, Acc: 0.95\n",
      "Step [22500/50000], Loss: 0.2921, Acc: 0.91\n",
      "Step [22600/50000], Loss: 0.1822, Acc: 0.95\n",
      "Step [22700/50000], Loss: 0.1533, Acc: 0.97\n",
      "Step [22800/50000], Loss: 0.1357, Acc: 0.96\n",
      "Step [22900/50000], Loss: 0.1301, Acc: 0.94\n",
      "Step [23000/50000], Loss: 0.2778, Acc: 0.91\n",
      "Step [23100/50000], Loss: 0.3093, Acc: 0.94\n",
      "Step [23200/50000], Loss: 0.3382, Acc: 0.90\n",
      "Step [23300/50000], Loss: 0.1946, Acc: 0.94\n",
      "Step [23400/50000], Loss: 0.1367, Acc: 0.97\n",
      "Step [23500/50000], Loss: 0.2099, Acc: 0.93\n",
      "Step [23600/50000], Loss: 0.1715, Acc: 0.95\n",
      "Step [23700/50000], Loss: 0.1988, Acc: 0.95\n",
      "Step [23800/50000], Loss: 0.2000, Acc: 0.94\n",
      "Step [23900/50000], Loss: 0.1957, Acc: 0.97\n",
      "Step [24000/50000], Loss: 0.1750, Acc: 0.96\n",
      "Step [24100/50000], Loss: 0.0951, Acc: 0.97\n",
      "Step [24200/50000], Loss: 0.2576, Acc: 0.91\n",
      "Step [24300/50000], Loss: 0.2154, Acc: 0.95\n",
      "Step [24400/50000], Loss: 0.2278, Acc: 0.93\n",
      "Step [24500/50000], Loss: 0.2828, Acc: 0.93\n",
      "Step [24600/50000], Loss: 0.1295, Acc: 0.98\n",
      "Step [24700/50000], Loss: 0.1714, Acc: 0.95\n",
      "Step [24800/50000], Loss: 0.2948, Acc: 0.89\n",
      "Step [24900/50000], Loss: 0.2072, Acc: 0.95\n",
      "Step [25000/50000], Loss: 0.1348, Acc: 0.98\n",
      "Step [25100/50000], Loss: 0.1759, Acc: 0.98\n",
      "Step [25200/50000], Loss: 0.1763, Acc: 0.95\n",
      "Step [25300/50000], Loss: 0.1242, Acc: 0.97\n",
      "Step [25400/50000], Loss: 0.1280, Acc: 0.97\n",
      "Step [25500/50000], Loss: 0.1736, Acc: 0.95\n",
      "Step [25600/50000], Loss: 0.2915, Acc: 0.92\n",
      "Step [25700/50000], Loss: 0.1857, Acc: 0.94\n",
      "Step [25800/50000], Loss: 0.1863, Acc: 0.94\n",
      "Step [25900/50000], Loss: 0.2475, Acc: 0.94\n",
      "Step [26000/50000], Loss: 0.1595, Acc: 0.96\n",
      "Step [26100/50000], Loss: 0.1482, Acc: 0.97\n",
      "Step [26200/50000], Loss: 0.1931, Acc: 0.93\n",
      "Step [26300/50000], Loss: 0.1664, Acc: 0.96\n",
      "Step [26400/50000], Loss: 0.1518, Acc: 0.96\n",
      "Step [26500/50000], Loss: 0.2005, Acc: 0.95\n",
      "Step [26600/50000], Loss: 0.1557, Acc: 0.95\n",
      "Step [26700/50000], Loss: 0.2066, Acc: 0.94\n",
      "Step [26800/50000], Loss: 0.2522, Acc: 0.94\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-76946dcbfb12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Fetch the images and labels and convert them to variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_var\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_var\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\Anaconda3\\envs\\PyTorchEnv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    188\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\Anaconda3\\envs\\PyTorchEnv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    188\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\Anaconda3\\envs\\PyTorchEnv\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\Anaconda3\\envs\\PyTorchEnv\\lib\\site-packages\\torchvision\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "for step in range(total_step):\n",
    "    \n",
    "    # Reset the data_iter\n",
    "    if (step+1) % iter_per_epoch == 0:\n",
    "        data_iter = iter(data_loader)\n",
    "\n",
    "    # Fetch the images and labels and convert them to variables\n",
    "    images, labels = next(data_iter)\n",
    "    images, labels = to_var(images.view(images.size(0), -1)), to_var(labels)\n",
    "    \n",
    "    # Forward, backward and optimize\n",
    "    optimizer.zero_grad()  # zero the gradient buffer\n",
    "    outputs = net(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Compute accuracy\n",
    "    _, argmax = torch.max(outputs, 1)\n",
    "    accuracy = (labels == argmax.squeeze()).float().mean()\n",
    "\n",
    "    if (step+1) % 100 == 0:\n",
    "        print ('Step [%d/%d], Loss: %.4f, Acc: %.2f' \n",
    "               %(step+1, total_step, loss.data[0], accuracy.data[0]))\n",
    "\n",
    "        #============ TensorBoard logging ============#\n",
    "        # (1) Log the scalar values\n",
    "        info = {\n",
    "            'loss': loss.data[0],\n",
    "            'accuracy': accuracy.data[0]\n",
    "        }\n",
    "\n",
    "        for tag, value in info.items():\n",
    "            logger.scalar_summary(tag, value, step+1)\n",
    "\n",
    "        # (2) Log values and gradients of the parameters (histogram)\n",
    "        for tag, value in net.named_parameters():\n",
    "            tag = tag.replace('.', '/')\n",
    "            logger.histo_summary(tag, to_np(value), step+1)\n",
    "            logger.histo_summary(tag+'/grad', to_np(value.grad), step+1)\n",
    "\n",
    "        # (3) Log the images\n",
    "        info = {\n",
    "            'images': to_np(images.view(-1, 28, 28)[:10])\n",
    "        }\n",
    "\n",
    "        for tag, images in info.items():\n",
    "            logger.image_summary(tag, images, step+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
