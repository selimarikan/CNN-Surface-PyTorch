{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets \n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_var(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Image processing \n",
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.5, 0.5, 0.5), \n",
    "                                     std=(0.5, 0.5, 0.5))])\n",
    "# MNIST dataset\n",
    "mnist = datasets.MNIST(root='./data/',\n",
    "                       train=True,\n",
    "                       transform=transform,\n",
    "                       download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist,\n",
    "                                          batch_size=100, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "D = nn.Sequential(\n",
    "    nn.Linear(784, 256),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(256, 256),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(256, 1),\n",
    "    nn.Sigmoid())\n",
    "\n",
    "# Generator \n",
    "G = nn.Sequential(\n",
    "    nn.Linear(64, 256),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(256, 256),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(256, 784),\n",
    "    nn.Tanh())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    D.cuda()\n",
    "    G.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Binary cross entropy loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0003)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/200], Step[300/600], d_loss: 0.2668, g_loss: 5.8942, D(x): 0.94, D(G(z)): 0.09\n",
      "Epoch [0/200], Step[600/600], d_loss: 0.0904, g_loss: 5.0523, D(x): 0.95, D(G(z)): 0.02\n",
      "Epoch [1/200], Step[300/600], d_loss: 0.6151, g_loss: 4.1212, D(x): 0.87, D(G(z)): 0.27\n",
      "Epoch [1/200], Step[600/600], d_loss: 0.6187, g_loss: 3.2803, D(x): 0.82, D(G(z)): 0.19\n",
      "Epoch [2/200], Step[300/600], d_loss: 0.7462, g_loss: 1.9519, D(x): 0.75, D(G(z)): 0.30\n",
      "Epoch [2/200], Step[600/600], d_loss: 0.5329, g_loss: 2.5638, D(x): 0.82, D(G(z)): 0.22\n",
      "Epoch [3/200], Step[300/600], d_loss: 0.7772, g_loss: 1.7979, D(x): 0.73, D(G(z)): 0.21\n",
      "Epoch [3/200], Step[600/600], d_loss: 0.4610, g_loss: 2.0244, D(x): 0.84, D(G(z)): 0.18\n",
      "Epoch [4/200], Step[300/600], d_loss: 1.3985, g_loss: 1.1188, D(x): 0.62, D(G(z)): 0.42\n",
      "Epoch [4/200], Step[600/600], d_loss: 1.4541, g_loss: 2.3461, D(x): 0.72, D(G(z)): 0.47\n",
      "Epoch [5/200], Step[300/600], d_loss: 0.2211, g_loss: 3.7069, D(x): 0.92, D(G(z)): 0.10\n",
      "Epoch [5/200], Step[600/600], d_loss: 0.9180, g_loss: 1.8218, D(x): 0.72, D(G(z)): 0.27\n",
      "Epoch [6/200], Step[300/600], d_loss: 0.4148, g_loss: 2.4813, D(x): 0.88, D(G(z)): 0.16\n",
      "Epoch [6/200], Step[600/600], d_loss: 0.3833, g_loss: 2.7576, D(x): 0.86, D(G(z)): 0.15\n",
      "Epoch [7/200], Step[300/600], d_loss: 0.3217, g_loss: 2.7764, D(x): 0.90, D(G(z)): 0.12\n",
      "Epoch [7/200], Step[600/600], d_loss: 0.6542, g_loss: 3.5772, D(x): 0.82, D(G(z)): 0.20\n",
      "Epoch [8/200], Step[300/600], d_loss: 0.4370, g_loss: 2.7294, D(x): 0.89, D(G(z)): 0.21\n",
      "Epoch [8/200], Step[600/600], d_loss: 0.6352, g_loss: 2.8347, D(x): 0.80, D(G(z)): 0.15\n",
      "Epoch [9/200], Step[300/600], d_loss: 0.8672, g_loss: 1.8190, D(x): 0.78, D(G(z)): 0.30\n",
      "Epoch [9/200], Step[600/600], d_loss: 0.5460, g_loss: 2.3299, D(x): 0.82, D(G(z)): 0.18\n",
      "Epoch [10/200], Step[300/600], d_loss: 0.5548, g_loss: 2.7293, D(x): 0.77, D(G(z)): 0.13\n",
      "Epoch [10/200], Step[600/600], d_loss: 0.7416, g_loss: 2.0581, D(x): 0.79, D(G(z)): 0.26\n",
      "Epoch [11/200], Step[300/600], d_loss: 0.4984, g_loss: 2.1667, D(x): 0.82, D(G(z)): 0.20\n",
      "Epoch [11/200], Step[600/600], d_loss: 0.9983, g_loss: 1.5435, D(x): 0.72, D(G(z)): 0.35\n",
      "Epoch [12/200], Step[300/600], d_loss: 0.6447, g_loss: 2.3347, D(x): 0.79, D(G(z)): 0.19\n",
      "Epoch [12/200], Step[600/600], d_loss: 0.8153, g_loss: 2.3707, D(x): 0.78, D(G(z)): 0.22\n",
      "Epoch [13/200], Step[300/600], d_loss: 0.9328, g_loss: 2.4634, D(x): 0.79, D(G(z)): 0.35\n",
      "Epoch [13/200], Step[600/600], d_loss: 0.9804, g_loss: 2.7985, D(x): 0.72, D(G(z)): 0.27\n",
      "Epoch [14/200], Step[300/600], d_loss: 0.9600, g_loss: 2.2101, D(x): 0.72, D(G(z)): 0.28\n",
      "Epoch [14/200], Step[600/600], d_loss: 1.1461, g_loss: 1.3738, D(x): 0.76, D(G(z)): 0.43\n",
      "Epoch [15/200], Step[300/600], d_loss: 0.5958, g_loss: 3.0386, D(x): 0.85, D(G(z)): 0.24\n",
      "Epoch [15/200], Step[600/600], d_loss: 0.6731, g_loss: 3.0984, D(x): 0.79, D(G(z)): 0.23\n",
      "Epoch [16/200], Step[300/600], d_loss: 0.8042, g_loss: 1.8505, D(x): 0.73, D(G(z)): 0.28\n",
      "Epoch [16/200], Step[600/600], d_loss: 1.0457, g_loss: 3.1880, D(x): 0.72, D(G(z)): 0.28\n",
      "Epoch [17/200], Step[300/600], d_loss: 1.4250, g_loss: 1.6569, D(x): 0.60, D(G(z)): 0.35\n",
      "Epoch [17/200], Step[600/600], d_loss: 0.9040, g_loss: 1.9184, D(x): 0.77, D(G(z)): 0.30\n",
      "Epoch [18/200], Step[300/600], d_loss: 0.7448, g_loss: 2.5958, D(x): 0.73, D(G(z)): 0.22\n",
      "Epoch [18/200], Step[600/600], d_loss: 1.2713, g_loss: 1.2240, D(x): 0.62, D(G(z)): 0.39\n",
      "Epoch [19/200], Step[300/600], d_loss: 0.9059, g_loss: 1.4606, D(x): 0.74, D(G(z)): 0.32\n",
      "Epoch [19/200], Step[600/600], d_loss: 1.5466, g_loss: 1.5811, D(x): 0.53, D(G(z)): 0.29\n",
      "Epoch [20/200], Step[300/600], d_loss: 0.7673, g_loss: 1.6745, D(x): 0.75, D(G(z)): 0.27\n",
      "Epoch [20/200], Step[600/600], d_loss: 1.1080, g_loss: 1.6842, D(x): 0.64, D(G(z)): 0.30\n",
      "Epoch [21/200], Step[300/600], d_loss: 0.7949, g_loss: 1.8949, D(x): 0.77, D(G(z)): 0.27\n",
      "Epoch [21/200], Step[600/600], d_loss: 0.7634, g_loss: 1.8492, D(x): 0.79, D(G(z)): 0.30\n",
      "Epoch [22/200], Step[300/600], d_loss: 0.9272, g_loss: 1.8924, D(x): 0.70, D(G(z)): 0.28\n",
      "Epoch [22/200], Step[600/600], d_loss: 0.8106, g_loss: 2.1746, D(x): 0.81, D(G(z)): 0.33\n",
      "Epoch [23/200], Step[300/600], d_loss: 1.0326, g_loss: 1.6284, D(x): 0.65, D(G(z)): 0.30\n",
      "Epoch [23/200], Step[600/600], d_loss: 0.9227, g_loss: 2.3061, D(x): 0.71, D(G(z)): 0.23\n",
      "Epoch [24/200], Step[300/600], d_loss: 0.8877, g_loss: 1.6883, D(x): 0.72, D(G(z)): 0.30\n",
      "Epoch [24/200], Step[600/600], d_loss: 0.8679, g_loss: 2.4899, D(x): 0.70, D(G(z)): 0.26\n",
      "Epoch [25/200], Step[300/600], d_loss: 0.9987, g_loss: 1.9917, D(x): 0.67, D(G(z)): 0.28\n",
      "Epoch [25/200], Step[600/600], d_loss: 0.9444, g_loss: 1.9547, D(x): 0.74, D(G(z)): 0.33\n",
      "Epoch [26/200], Step[300/600], d_loss: 1.0794, g_loss: 1.6621, D(x): 0.71, D(G(z)): 0.39\n",
      "Epoch [26/200], Step[600/600], d_loss: 1.1078, g_loss: 1.4790, D(x): 0.70, D(G(z)): 0.39\n",
      "Epoch [27/200], Step[300/600], d_loss: 1.1225, g_loss: 1.2433, D(x): 0.66, D(G(z)): 0.36\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5d7527160f7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mg_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0mg_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\Anaconda3\\envs\\PyTorchEnv\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"gradient has to be a Tensor, Variable or None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "for epoch in range(200):\n",
    "    for i, (images, _) in enumerate(data_loader):\n",
    "        # Build mini-batch dataset\n",
    "        batch_size = images.size(0)\n",
    "        images = to_var(images.view(batch_size, -1))\n",
    "        real_labels = to_var(torch.ones(batch_size))\n",
    "        fake_labels = to_var(torch.zeros(batch_size))\n",
    "\n",
    "        #============= Train the discriminator =============#\n",
    "        # Compute loss with real images\n",
    "        outputs = D(images)\n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "        real_score = outputs\n",
    "        \n",
    "        # Compute loss with fake images\n",
    "        z = to_var(torch.randn(batch_size, 64))\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images)\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "        fake_score = outputs\n",
    "        \n",
    "        # Backprop + Optimize\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        D.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        #=============== Train the generator ===============#\n",
    "        # Compute loss with fake images\n",
    "        z = to_var(torch.randn(batch_size, 64))\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images)\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        \n",
    "        # Backprop + Optimize\n",
    "        D.zero_grad()\n",
    "        G.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        if (i+1) % 300 == 0:\n",
    "            print('Epoch [%d/%d], Step[%d/%d], d_loss: %.4f, '\n",
    "                  'g_loss: %.4f, D(x): %.2f, D(G(z)): %.2f' \n",
    "                  %(epoch, 200, i+1, 600, d_loss.data[0], g_loss.data[0],\n",
    "                    real_score.data.mean(), fake_score.data.mean()))\n",
    "    \n",
    "    # Save real images\n",
    "    if (epoch+1) == 1:\n",
    "        images = images.view(images.size(0), 1, 28, 28)\n",
    "        save_image(denorm(images.data), './data/real_images.png')\n",
    "    \n",
    "    # Save sampled images\n",
    "    fake_images = fake_images.view(fake_images.size(0), 1, 28, 28)\n",
    "    save_image(denorm(fake_images.data), './data/fake_images-%d.png' %(epoch+1))\n",
    "\n",
    "# Save the trained parameters \n",
    "torch.save(G.state_dict(), './generator.pkl')\n",
    "torch.save(D.state_dict(), './discriminator.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
